---
authors: "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
year: 2019
citekey: liuRoBERTaRobustlyOptimized2019
---

# RoBERTa: A Robustly Optimized BERT Pretraining Approach
> **TL;DR**:  Robustly optimized BERT approach: nhận thấy BERT bị **undertrained**, RoBERTa with carefully hyperparameters chosen (beta) + bigger (size, BPE,  batch size, training step, corpus) + remove NSP outperform BERT on all task

## Highlight
- ![](./static/images/2021-05-12-14-57-47.png)