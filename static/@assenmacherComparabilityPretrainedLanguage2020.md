---
title: "On the comparability of Pre-trained Language Models"
authors: "Matthias Aßenmacher, Christian Heumann"
year: 2020
citekey: assenmacherComparabilityPretrainedLanguage2020
---

# On the comparability of Pre-trained Language Models
> **TL;DR**:  Một paper literature review về so sánh các SOTA LM gần đây, dựa trên 3 yếu tố: Dataset, Model architecture (gồm kiến trúc + pretraining task) và computational resource. Tuy nhiên chưa đi sâu và còn thiếu các kiến trúc transformer mới (PEGASUS, CTRL, ELECTRA, ...)

- Nhận xét:
  - Có tổng hợp các bộ dữ liệu được đánh giá và so sánh giữa các model

## Highlight
![](static/images/Screen Shot 2021-05-06 at 16.46.41.png)

![](static/images/Screen Shot 2021-05-06 at 16.47.09.png)