---
title: "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
authors: "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
year: 2019
citekey: liuRoBERTaRobustlyOptimized2019
---

# RoBERTa: A Robustly Optimized BERT Pretraining Approach
> **TL;DR**:  Robustly optimized BERT approach: nhận thấy BERT bị **undertrained**, RoBERTa with carefully hyperparameters chosen (beta) + bigger (size, BPE,  batch size, training step, corpus) + remove NSP outperform BERT on all task

## Highlight
- ![[Screen Shot 2021-05-12 at 14.57.47.png]]