{"Graph":{"adjacencyMap":{"@baoUniLMv2PseudoMaskedLanguage2020":{},"@yangXLNetGeneralizedAutoregressive2020":{},"@lampleCrosslingualLanguageModel2019":{},"@daiTransformerXLAttentiveLanguage2019":{},"@khandelwalGENERALIZATIONMEMORIZATIONNEAREST2020":{},"@liuRoBERTaRobustlyOptimized2019":{},"@zhangUnderstandingDeepLearning2017":{},"README":{"papers":["cf",[]]},"@radfordLanguageModelsAre":{"@carliniExtractingTrainingData2020":["cf",[]]},"@keskarCTRLConditionalTransformer2019":{},"@wenzekCCNetExtractingHigh2019":{},"@radfordImprovingLanguageUnderstanding":{},"@carliniExtractingTrainingData2020":{},"@holtzmanCuriousCaseNeural2020a":{},"@lewisBARTDenoisingSequencetoSequence2019":{},"@joshiSpanBERTImprovingPretraining2020":{"@lewisBARTDenoisingSequencetoSequence2019":["cf",[]]},"@clarkELECTRAPretrainingText2020":{},"papers":{"@baoUniLMv2PseudoMaskedLanguage2020":["cf",[]],"@yangXLNetGeneralizedAutoregressive2020":["cf",[]],"@lampleCrosslingualLanguageModel2019":["cf",[]],"@daiTransformerXLAttentiveLanguage2019":["cf",[]],"@khandelwalGENERALIZATIONMEMORIZATIONNEAREST2020":["cf",[]],"@liuRoBERTaRobustlyOptimized2019":["cf",[]],"@zhangUnderstandingDeepLearning2017":["cf",[]],"@radfordLanguageModelsAre":["cf",[]],"@keskarCTRLConditionalTransformer2019":["cf",[]],"@wenzekCCNetExtractingHigh2019":["cf",[]],"@radfordImprovingLanguageUnderstanding":["cf",[]],"@carliniExtractingTrainingData2020":["cf",[]],"@holtzmanCuriousCaseNeural2020a":["cf",[]],"@lewisBARTDenoisingSequencetoSequence2019":["cf",[]],"@joshiSpanBERTImprovingPretraining2020":["cf",[]],"@clarkELECTRAPretrainingText2020":["cf",[]],"@assenmacherComparabilityPretrainedLanguage2020":["cf",[]],"@dongUnifiedLanguageModel2019":["cf",[]]},"@koltunMeasureResearchTaste2021":{},"index":{"README":["cf",[]]},"@assenmacherComparabilityPretrainedLanguage2020":{},"@dongUnifiedLanguageModel2019":{}},"vertices":{"@baoUniLMv2PseudoMaskedLanguage2020":{"Path":"./posts/@baoUniLMv2PseudoMaskedLanguage2020.md","Slug":"@baoUniLMv2PseudoMaskedLanguage2020","ID":"@baoUniLMv2PseudoMaskedLanguage2020","Meta":{"authors":"Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon","citekey":"baoUniLMv2PseudoMaskedLanguage2020","year":2020,"tags":[]},"Title":"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"},"@yangXLNetGeneralizedAutoregressive2020":{"Path":"./posts/@yangXLNetGeneralizedAutoregressive2020.md","Slug":"@yangXLNetGeneralizedAutoregressive2020","ID":"@yangXLNetGeneralizedAutoregressive2020","Meta":{"authors":"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le","citekey":"yangXLNetGeneralizedAutoregressive2020","year":2020,"tags":[]},"Title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},"@lampleCrosslingualLanguageModel2019":{"Path":"./posts/@lampleCrosslingualLanguageModel2019.md","Slug":"@lampleCrosslingualLanguageModel2019","ID":"@lampleCrosslingualLanguageModel2019","Meta":{"authors":"Guillaume Lample, Alexis Conneau","citekey":"lampleCrosslingualLanguageModel2019","year":2019,"tags":[]},"Title":"Cross-lingual Language Model Pretraining"},"@daiTransformerXLAttentiveLanguage2019":{"Path":"./posts/@daiTransformerXLAttentiveLanguage2019.md","Slug":"@daiTransformerXLAttentiveLanguage2019","ID":"@daiTransformerXLAttentiveLanguage2019","Meta":{"authors":"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov","citekey":"daiTransformerXLAttentiveLanguage2019","year":2019,"tags":["RECL"]},"Title":"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"},"@khandelwalGENERALIZATIONMEMORIZATIONNEAREST2020":{"Path":"./posts/@khandelwalGENERALIZATIONMEMORIZATIONNEAREST2020.md","Slug":"@khandelwalGENERALIZATIONMEMORIZATIONNEAREST2020","ID":"@khandelwalGENERALIZATIONMEMORIZATIONNEAREST2020","Meta":{"authors":"Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis","citekey":"khandelwalGENERALIZATIONMEMORIZATIONNEAREST2020","year":2019,"tags":[]},"Title":"GENERALIZATION THROUGH MEMORIZATION: NEAREST NEIGHBOR LANGUAGE MODELS"},"@liuRoBERTaRobustlyOptimized2019":{"Path":"./posts/@liuRoBERTaRobustlyOptimized2019.md","Slug":"@liuRoBERTaRobustlyOptimized2019","ID":"@liuRoBERTaRobustlyOptimized2019","Meta":{"authors":"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov","citekey":"liuRoBERTaRobustlyOptimized2019","year":2019,"tags":[]},"Title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},"@zhangUnderstandingDeepLearning2017":{"Path":"./posts/@zhangUnderstandingDeepLearning2017.md","Slug":"@zhangUnderstandingDeepLearning2017","ID":"@zhangUnderstandingDeepLearning2017","Meta":{"authors":"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals","citekey":"zhangUnderstandingDeepLearning2017","year":2017,"tags":[]},"Title":"Understanding deep learning requires rethinking generalization"},"README":{"Path":"./README.md","Slug":"README","ID":"README","Meta":{"tags":[]},"Title":"Thanh’s index"},"@radfordLanguageModelsAre":{"Path":"./posts/@radfordLanguageModelsAre.md","Slug":"@radfordLanguageModelsAre","ID":"@radfordLanguageModelsAre","Meta":{"authors":"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever","citekey":"radfordLanguageModelsAre","year":null,"tags":[]},"Title":"Language Models are Unsupervised Multitask Learners"},"@keskarCTRLConditionalTransformer2019":{"Path":"./posts/@keskarCTRLConditionalTransformer2019.md","Slug":"@keskarCTRLConditionalTransformer2019","ID":"@keskarCTRLConditionalTransformer2019","Meta":{"authors":"Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, Richard Socher","citekey":"keskarCTRLConditionalTransformer2019","year":2019,"tags":[]},"Title":"CTRL: A Conditional Transformer Language Model for Controllable Generation"},"@wenzekCCNetExtractingHigh2019":{"Path":"./posts/@wenzekCCNetExtractingHigh2019.md","Slug":"@wenzekCCNetExtractingHigh2019","ID":"@wenzekCCNetExtractingHigh2019","Meta":{"authors":"Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, Edouard Grave","citekey":"wenzekCCNetExtractingHigh2019","year":2019,"tags":[]},"Title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},"@radfordImprovingLanguageUnderstanding":{"Path":"./posts/@radfordImprovingLanguageUnderstanding.md","Slug":"@radfordImprovingLanguageUnderstanding","ID":"@radfordImprovingLanguageUnderstanding","Meta":{"authors":"Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever","citekey":"radfordImprovingLanguageUnderstanding","year":null,"tags":[]},"Title":"Improving Language Understanding by Generative Pre-Training"},"@carliniExtractingTrainingData2020":{"Path":"./posts/@carliniExtractingTrainingData2020.md","Slug":"@carliniExtractingTrainingData2020","ID":"@carliniExtractingTrainingData2020","Meta":{"authors":"Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel","citekey":"carliniExtractingTrainingData2020","year":2020,"tags":[]},"Title":"Extracting Training Data from Large Language Models"},"@holtzmanCuriousCaseNeural2020a":{"Path":"./posts/@holtzmanCuriousCaseNeural2020a.md","Slug":"@holtzmanCuriousCaseNeural2020a","ID":"@holtzmanCuriousCaseNeural2020a","Meta":{"authors":"Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi","citekey":"holtzmanCuriousCaseNeural2020a","year":2020,"tags":[]},"Title":"The Curious Case of Neural Text Degeneration"},"@lewisBARTDenoisingSequencetoSequence2019":{"Path":"./posts/@lewisBARTDenoisingSequencetoSequence2019.md","Slug":"@lewisBARTDenoisingSequencetoSequence2019","ID":"@lewisBARTDenoisingSequencetoSequence2019","Meta":{"authors":"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer","citekey":"lewisBARTDenoisingSequencetoSequence2019","year":2019,"tags":[]},"Title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},"@joshiSpanBERTImprovingPretraining2020":{"Path":"./posts/@joshiSpanBERTImprovingPretraining2020.md","Slug":"@joshiSpanBERTImprovingPretraining2020","ID":"@joshiSpanBERTImprovingPretraining2020","Meta":{"authors":"Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy","citekey":"joshiSpanBERTImprovingPretraining2020","year":2020,"tags":[]},"Title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},"@clarkELECTRAPretrainingText2020":{"Path":"./posts/@clarkELECTRAPretrainingText2020.md","Slug":"@clarkELECTRAPretrainingText2020","ID":"@clarkELECTRAPretrainingText2020","Meta":{"authors":"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning","citekey":"clarkELECTRAPretrainingText2020","year":2020,"tags":[]},"Title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},"papers":{"Path":"./papers.md","Slug":"papers","ID":"papers","Meta":{"tags":[]},"Title":"Paper reading"},"@koltunMeasureResearchTaste2021":{"Path":"./posts/@koltunMeasureResearchTaste2021.md","Slug":"@koltunMeasureResearchTaste2021","ID":"@koltunMeasureResearchTaste2021","Meta":{"authors":"Vladlen Koltun, David Hafner","citekey":"koltunMeasureResearchTaste2021","year":2021,"tags":[]},"Title":"A Measure of Research Taste"},"index":{"Path":"./index.md","Slug":"index","ID":"index","Meta":{"tags":[]},"Title":"Thanh’s Blog"},"@assenmacherComparabilityPretrainedLanguage2020":{"Path":"./posts/@assenmacherComparabilityPretrainedLanguage2020.md","Slug":"@assenmacherComparabilityPretrainedLanguage2020","ID":"@assenmacherComparabilityPretrainedLanguage2020","Meta":{"authors":"Matthias Aßenmacher, Christian Heumann","citekey":"assenmacherComparabilityPretrainedLanguage2020","year":2020,"tags":[]},"Title":"On the comparability of Pre-trained Language Models"},"@dongUnifiedLanguageModel2019":{"Path":"./posts/@dongUnifiedLanguageModel2019.md","Slug":"@dongUnifiedLanguageModel2019","ID":"@dongUnifiedLanguageModel2019","Meta":{"authors":"Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon","citekey":"dongUnifiedLanguageModel2019","year":2019,"tags":[]},"Title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"}}},"NeuronVersion":"1.9.31.0","Config":{"editUrl":"https://github.com/luvata/blogs/edit/master/","plugins":["neuronignore","links","tags","uptree","feed"],"siteBaseUrl":"https://luvata.github.io/blogs/","author":"Thanh Le","siteTitle":"Luvata's blog","theme":"black"},"Errors":{"README":{"tag":"ZettelIssue_MissingLinks","contents":["README",["posts"]]}}}