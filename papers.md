# Paper reading

- [[@clarkELECTRAPretrainingText2020.md]]
- [[@yangXLNetGeneralizedAutoregressive2020.md]]
- [[@daiTransformerXLAttentiveLanguage2019.md]]
- [[@joshiSpanBERTImprovingPretraining2020.md]]
- [[@dewynterOptimalSubarchitectureExtraction2020.md]]
- [[@lampleCrosslingualLanguageModel2019.md]]
- [[@lewisBARTDenoisingSequencetoSequence2019.md]]
- [[@liuRoBERTaRobustlyOptimized2019.md]]
- [[@baoUniLMv2PseudoMaskedLanguage2020.md]]
- [[@dongUnifiedLanguageModel2019.md]]
- [[@holtzmanCuriousCaseNeural2020a.md]]
- [[@zhangUnderstandingDeepLearning2017.md]]
- [[@wenzekCCNetExtractingHigh2019.md]]
- [[@assenmacherComparabilityPretrainedLanguage2020.md]]
- [[@radfordLanguageModelsAre.md]]
- [[@radfordImprovingLanguageUnderstanding.md]]
- [[@carliniExtractingTrainingData2020.md]]
- [[@khandelwalGENERALIZATIONMEMORIZATIONNEAREST2020.md]]
- [[@keskarCTRLConditionalTransformer2019]]
